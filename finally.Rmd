---
title: "LSTM_up"
output: html_notebook
---
#先讀取資料
```{r}
tw1101<-read.csv( file = "D://stockdata/tw1101.csv")
tw2002<-read.csv( file = "D://stockdata/tw2002.csv")
tw2327<-read.csv( file = "D://stockdata/tw2327.csv")
tw2330<-read.csv( file = "D://stockdata/tw2330.csv")
tw2412<-read.csv( file = "D://stockdata/tw2412.csv")
tw2610<-read.csv( file = "D://stockdata/tw2610.csv")
tw3008<-read.csv( file = "D://stockdata/tw3008.csv")
allstock<-list(tw1101=tw1101,tw2002=tw2002,tw2327=tw2327,tw2330=tw2330,tw2412=tw2412,tw2610=tw2610,tw3008=tw3008)
stock_names<-c("tw1101","tw2002","tw2327","tw2330","tw2412","tw2610","tw3008")
```
#檢查資料是否有遺漏值
```{r}
summary(tw2330)
prime <- function(x){
     if(x==1) return ("NULL")
     y <- c()
     a <- c(2:x)
     b <- min(a[which(x%%a==0)])
    x<-x/b
     if(x==1) return ("NULL")
     y <-c(y,b)
     while(!x==1){
         
         a <- c(2:x)
         b <- min(a[which(x%%a==0)])
         x<-x/b
         y <- c(y,b)
     }
     return(y)
 }#找質因數
for (i in 1:7) {
  print(prime(dim(allstock[[i]])[1]-23))
}
prime(2044)
```
#載入package
```{r}
library(tidyverse)
library(glue)
library(forcats)

# Time Series
library(timetk)
library(tidyquant)
library(tibbletime)

# Visualization
library(cowplot)

# Preprocessing
library(recipes)

# Sampling / Accuracy
library(rsample)
library(yardstick) 

# Modeling
library(keras)
library(tensorflow)
```
#計算lstm_tw2330
```{r}
set.seed(10)
tw2330<-read.csv( file = "D://stockdata/tw2330.csv")
LSTM<-function(dataset){
diffed<-diff(dataset,differences = 1)#前後兩天的股價相差
lag_transform <- function(x, k= 1){
      lagged =  c(rep(NA, k), x[1:(length(x)-k)])
      DF = as.data.frame(cbind(lagged, x))
      colnames(DF) <- c( paste0('x-', k), 'x')
      DF[is.na(DF)] <- 0
      return(DF)
}
  supervised = lag_transform(diffed,1)
  supervised[(dim(supervised)[1]+1),]<-c(supervised[dim(supervised)[1],2],0)
  supervised<-supervised[2:dim(supervised)[1],]
  row.names(supervised)<-1:dim(supervised)[1]
  N = nrow(supervised)#總共data
  n = round(N *0.8, digits = 0)
  train = supervised[1:n, ]#traindata
  test  = supervised[(n+1):N,]#testdata
  scale_data=function(train, test, feature_range = c(0, 1)) {
   x = train
   fr_min = feature_range[1]
   fr_max = feature_range[2]
   std_train = ((x - min(x) ) / (max(x) - min(x)  ))
   std_test  = ((test - min(x) ) / (max(x) - min(x)  ))
   scaled_train = std_train *(fr_max -fr_min) + fr_min
   scaled_test = std_test *(fr_max -fr_min) + fr_min
   return(list(scaled_train = as.vector(scaled_train), scaled_test = as.vector(scaled_test) ,scaler= c(min =min(x), max = max(x))) )
 }
 Scaled = scale_data(train, test, c(-1, 1))
 y_train = Scaled$scaled_train[, 2]#train 目標差x
 x_train = Scaled$scaled_train[, 1]#train 慢一天的差x-1
 y_test = Scaled$scaled_test[, 2]#test 慢一天的差x
 x_test = Scaled$scaled_test[, 1]#test 慢一天的差x-1
 invert_scaling = function(scaled, scaler, feature_range = c(0, 1)){
   min = Scaled$scaler[1]
   max = Scaled$scaler[2]
   t = length(scaled)
   mins = feature_range[1]
   maxs = feature_range[2]
   inverted_dfs = numeric(t)
   for(i in 1:t){
     X = (scaled[i]- mins)/(maxs - mins)
     rawValues = X *(max - min)+min
     inverted_dfs[i] <- rawValues
   }
  return(inverted_dfs)
 }
 dim(x_train) <- c(length(x_train), 1, 1)
 # specify required arguments
 X_shape2 = dim(x_train)[2]
 X_shape3 = dim(x_train)[3]
 batch_size = 7                # must be a common factor of both the train and test samples
 units = 1                     # can adjust this, in model tuninig phase
 model <- keras_model_sequential() 
 model%>%
   layer_lstm(units, batch_input_shape = c(batch_size, X_shape2, X_shape3), stateful= TRUE)%>%
   layer_dense(units = 1)
 model %>% compile(
   loss = 'mean_squared_error',
   optimizer = optimizer_adam( lr= 0.02, decay = 1e-6 ),  
   metrics = c('accuracy')
 )
 summary(model)
 Epochs = 100   
 for(i in 1:Epochs ){
   model %>% fit(x_train, y_train, epochs=1, batch_size=batch_size, verbose=1, shuffle=FALSE)
   model %>% reset_states()
 }
L = length(x_test)
scaler = Scaled$scaler
predictions = numeric(L)
for(i in 1:L){
      X = x_test[i:(i+6)]
      dim(X) = c(7,1,1)
      yhat = model %>% predict(X, batch_size=batch_size)
      #invert scaling
      yhat= invert_scaling(yhat[1], scaler,c(-1, 1))
      # invert differencing
      yhat  = yhat + dataset[(n+i+1)]
      # store
      predictions[i] <- yhat[1]
}
return(predictions)
}
tw2330<-read.csv( file = "D://stockdata/tw2330.csv")
tw2330_sma10<-LSTM(tw2330$sma10)#tw2330$sma10[2047:2556] vs tw2330_sma10[1:510]
tw2330_wma10<-LSTM(tw2330$wma10)#同上
tw2330_k_value<-LSTM(tw2330$k_value)
tw2330_d_value<-LSTM(tw2330$d_value)
tw2330_RSI9<-LSTM(tw2330$RSI.9)
tw2330_CCI14<-LSTM(tw2330$CCI14)
tw2330_R<-LSTM(tw2330$R)
tw2330_it<-LSTM(tw2330$it)
tw2330_de<-LSTM(tw2330$de)
tw2330_fi<-LSTM(tw2330$fi)
Mom<-momentum(tw2330$close, n = 9)#Momentum
Mom[1:9]<-c(-0.6,1.3,-1,-0.7,-1.85,-2.1,-2.4,-1.7,-1.4)
tw2330_MOm<-LSTM(Mom)
```
#統整技術指標
```{r}
library(TTR)
library(quantmod)
SMA_10<-c(tw2330$sma10[1:2046],tw2330_sma10[1:510])#SMA10
k<-c()
for (i in 2:length(SMA_10)) {
  if(SMA_10[i]>SMA_10[(i-1)]){
    k[i]<-1
  }else{
    k[i]<--1
  }
}
SMA_10<-k[2:length(SMA_10)]
WMA_10<-c(tw2330$wma10[1:2046],tw2330_wma10[1:510])#WMA10
k<-c()
for (i in 2:length(WMA_10)) {
  if(WMA_10[i]>WMA_10[(i-1)]){
    k[i]<-1
  }else{
    k[i]<--1
  }
}
WMA_10<-k[2:length(WMA_10)]
library(TTR)
Mom<-momentum(tw2330$close, n = 9)#Momentum
Mom[1:9]<-c(-0.6,1.3,-1,-0.7,-1.85,-2.1,-2.4,-1.7,-1.4)
k<-c(Mom[1:2046],tw2330_MOm[1:510])#MOm
Mom<-k
for (i in 1:length(Mom)) {
  if(Mom[i]>=0){
    k[i]<-1
  }else{
    k[i]<--1
  }
}
Mom<-k[2:length(Mom)]
k_value<-c(tw2330$k_value[1:2046],tw2330_k_value[1:510])#k_value
k<-c()
for (i in 2:length(k_value)) {
  if(k_value[i]>k_value[i-1]){
     k[i]<-1
   }else{
     k[i]<--1
   }
}
k_value1<-k[2:length(k_value)]
d_value<-c(tw2330$d_value[1:2046],tw2330_d_value[1:510])#d_value
k<-c()
for (i in 2:length(d_value)) {
  if(d_value[i]>d_value[i-1]){
     k[i]<-1
   }else{
     k[i]<--1
   }
}
d_value1<-k[2:length(d_value)]
rsi<-c(tw2330$RSI.9[1:2046],tw2330_RSI9[1:510])
k<-c()
for (i in 2:length(rsi)) {
  if(rsi[i]>80){
     k[i]<--1
   }else if(rsi[i]<20){
     k[i]<-1
   }else if(rsi[i]>rsi[i-1]){
     k[i]<-1
   }else{
     k[i]<--1
   }
}
rsi<-k[2:length(rsi)]
fi<-c(tw2330$fi[2:2046],tw2330_fi[1:510])
it<-c(tw2330$it[2:2046],tw2330_it[1:510])
de<-c(tw2330$de[2:2046],tw2330_de[1:510])
RT<-function(X){
  X1<-c()
  for (i in 1:length(X)) {
  if(X[i]>0){
    X1[i]<-1
  }else{
    X1[i]<--1
  }
  }
  return(X1)
}
fi<-RT(fi)
it<-RT(it)
de<-RT(de)
cci<-c(tw2330$CCI14[1:2046],tw2330_CCI14[1:510])
k<-c()
for (i in 2:length(cci)) {
  if(cci[i]>100){
     k[i]<--1
   }else if(cci[i]<(-100)){
     k[i]<-1
   }else if(cci[i]>cci[i-1]){
     k[i]<-1
   }else{
     k[i]<--1
   }
}
cci<-k[2:length(cci)]
WR<-c(tw2330$R[1:2046],tw2330_R[1:510])
k<-c()
for (i in 2:length(WR)) {
  if(WR[i]>WR[i-1]){
     k[i]<-1
   }else{
     k[i]<--1
   }
}
WR<-k[2:length(WR)]
TC<-scale(tw2330_TC)
for (i in 1:dim(TC)[2]) {
  TC[,i]<-as.factor(TC[,i])
}
```
#技術指標統整
```{r}
TC<-as.data.frame(cbind(SMA_10,WMA_10,Mom,rsi,WR,k_value1,d_value1,it,de,fi,cci))
tw2330_TC<-TC
y<-c()
for (i in 1:length(tw2330$diff)) {
  if(tw2330$diff[i]>0){
    y[i]<-1
  }else{
    y[i]<--1
  }
}
y<-c(y[2:2556])
tw2330_TC<-as.data.frame(tw2330_TC)
tw2330_TC<-cbind(tw2330_TC[1:2555,],y)
train<-tw2330_TC[1:2046,]
test<-tw2330_TC[2047:2555,]
train$y<-as.factor(train$y)
test$y<-as.factor(test$y)
```
#randomforest
```{r}
require(randomForest)
set.seed(10)
rf_model = randomForest(train$y~.,
                        data=train,
                        ntree=200,mtry=3)
# 預測
rf_testy = predict(rf_model,test[,-12])
rf_trainy = predict(rf_model,train[,-12])
r_test<-table(rf_testy,test$y)
r_train<-table(rf_trainy,train$y)
r_train
r_test
tuneRF(train[,-12],train$y)
```

